\chapter{\oneedge{}: Application orchestration over geo-distributed Edge infrastructure}
\label{sec:oneedge}

\section{Introduction}

\section{Basics of Application Orchestration in Edge Computing}

In this section, we will set the stage for the discussion of how \oneedge{} leverages the mechanisms proposed in this dissertation to implement control-plane policies for application orchestration. We first present the application model that \oneedge{} supports, the application requirements for which control-plane policies need to be designed, and the challenges faced by in an Edge setting due to the dynamism in workload.

\subsection{Application Model}
\label{sec:oneedge_app_model}
Situation-awareness applications process data incoming from sensors through a series of functions, each extracting out certain information from the input data or performing a certain operation. This can be naturally modeled as a Data Flow Graph (DFG) \cite{dfg}. Each node in a DFG represents a processing function and each directed edge represents a data dependency between the upstream and downstream node. In \oneedge{}, we assume a special case of the more general DFG model, which is a pipeline of functions. As shown in \cref{fig:app_pipeline}, each DFG node, or application component, is an independent actor, and is represented by a level. Level 0 is assigned to the most downstream component, and the level number increases as we go upstream toward the client. Each component reads from a queue of input events which is populated by the upstream component and sends output events to the downstream component. An application component is also able to send events to an upstream component (including the client).

\begin{figure}[ht]
\centering
\begin{subfigure}{.48\columnwidth}
  \centering
    \includegraphics[width=\columnwidth]{figures/oneedge/app_pipeline.pdf}
    \caption{Application Pipeline.}
    \label{fig:app_pipeline}
\end{subfigure}
\begin{subfigure}{.48\columnwidth}
  \centering
    \includegraphics[width=\columnwidth]{figures/oneedge/pipeline_tree.pdf}
    \caption{Actual deployment of a pipeline-based application model resembles a forest with multiple trees.}
    \label{fig:pipeline_tree}
\end{subfigure}
\caption{Description of the application model.}
\label{fig:app_model}
\end{figure}

Although applications are modeled and specified as a linear pipeline, upon deployment for multiple clients, world, the set of application components and the data dependencies among them resemble a forest, as shown in \cref{fig:pipeline_tree}. This is because in order to serve many geo-distributed clients, the same application component needs to have several instances deployed in network proximity of clients so that communication latency to the application instance can be minimized and real-time response made possible. However, not all pipeline components have stringent latency requirements, and thus can serve multiple clients. Each tree in the forest has the most downstream application component (with level 0) as the root node and clients as leaf nodes. Each root-to-leaf path in a tree is a complete application pipeline, and we call each such path except the leaf node an \textit{application instance}. An application component that serves more than one upstream components is essentially processing information gathered from multiple clients, and therefore is able to enable state sharing among clients. However, even for applications that don't share state among clients in their logic, sharing one or more components in the application pipeline among multiple clients can be beneficial. This is because the memory footprint of running multiple independent application components is higher than sharing an application component to serve the same number of clients \todo{quantify?}.

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\columnwidth]{figures/oneedge/collaborative_driving_app.pdf}
\caption{The collaborative driving assistance application modeled as a pipeline of components.}
\label{fig:collab_driving_pipeline}
\end{figure}

\par For example, the Collaborative Driving Assistance application can be modeled as a pipeline of application components as shown in \cref{fig:collab_driving_pipeline}. The Client component performs object detection on an input LiDAR sensor stream to generate a list of objects that it can see in its immediate field of view. The Sub-Regional View component aggregates the individual views from multiple vehicles that are in close spatial proximity of one another to create a composite view. This composite view is fed back to the vehicles so that they can improve their lane control and collision avoidance decisions. The output of Sub-Regional View components is processed by the Regional View component to perform higher-level analyses across a larger geographical area.

\begin{comment}
\subsubsection{Programming Model}
\begin{table}
\caption{Programming model API:  communication primitives.}
\label{table:comm_api}
\centering
\resizebox{0.95\textwidth}{!}{%
\begin{tabular}{|c|c|}
\hline
\textbf{Interface} & \textbf{Description} \\ \hline
\begin{tabular}{@{}c@{}} void send\_up (message m, edgeId o) \end{tabular} & \begin{tabular}{@{}c@{}} Sends a message asynchronously from a node \\ to the downstream node connected through edge \textit{o}. \end{tabular} \\ \hline
\begin{tabular}{@{}c@{}} void send\_down (message m, edgeId i,\\ \textbf{optional } nodeId n) \end{tabular} & \begin{tabular}{@{}c@{}}Sends a message asynchronously to all upstream \\nodes connected through edge \textit{i}.  Optionally it can \\ choose to only contact  one of the upstream nodes \textit{n}. \end{tabular} \\ \hline
\begin{tabular}{@{}c@{}} void send\_to (message m, \\nodeId destination) \end{tabular} & \begin{tabular}{@{}c@{}}Sends a message to a specific destination node.
\end{tabular} \\ \hline
\begin{tabular}{@{}c@{}} void send\_to\_partion\_clients (message m, \\partitionId id) \end{tabular} & \begin{tabular}{@{}c@{}}Sends a message to all the clients in a logical partition.
\end{tabular} \\ \hline
\end{tabular}
}
\end{table}
\end{comment}

\subsection{Control-Plane Policies for Application Orchestration}
Effective orchestration of situation-awareness applications on Edge infrastructure requires two main decisions that need to be taken by the control-plane. These two decisions are (i) mapping clients to application instances, and (ii) deploying and managing resources for application instances on Edge sites.

\subsubsection{Mapping Clients to Application Instances}
\newcommand{\clientset}{\mathcal{C}}
\newcommand{\instanceset}{\mathcal{I}}
\newcommand{\resourceset}{\mathcal{R}}

We now formally describe the decision concerning mapping clients to application instances for a specific application. Let $\clientset$ denote the set of clients for the given application and $\instanceset$ denote the set of running instances of all the components of this application. As mentioned before, each application component in $\instanceset$ as well as each client in $\clientset$ has an associated level, which indicates the stage number in the pipelined application model. The mapping of clients and upstream application components to downstream components is defined by a function $\mathcal{M}$, defined in \cref{eq:mapping_domains_eqns} and \cref{eq:mapping_eqn}.
\begin{equation}
\label{eq:mapping_domains_eqns}
\mathcal{M} : \left( \clientset \cup \instanceset \right) \times Z^+ \rightarrow \instanceset
\end{equation}

\begin{equation}
\label{eq:mapping_eqn}
\mathcal{M} \left( c, l \right) = \text{App Component of level }l \text{ serving }c
\end{equation}
The control-plane needs to compute such a mapping so as the ensure that the application requirements discussed in \cref{sec:oneedge_app_reqs} are satisfied.

\subsubsection{Deployment of Application Instances on Edge Sites}
Each application instance, comprising of all the components in the application's pipelined model, need to be deployed on a compute resource. The placement of application components ($\instanceset$) on the set of compute resources ($\resourceset$) can be represented by the function $\mathcal{P}$, as described in \cref{eq:placement_domain_eqn} and \cref{eq:placement_eqn}.

\begin{equation}
\label{eq:placement_domain_eqn}
\mathcal{P} : \instanceset \rightarrow \resourceset
\end{equation}

\begin{equation}
\label{eq:placement_eqn}
\mathcal{P} \left( a \right) = \text{Compute resource hosting }a
\end{equation}
The placement of application components is computed in a manner that meets the end-to-end latency requirements of the application, as described in \label{sec:oneedge_app_reqs}.

\subsection{Application Requirements}
\label{sec:oneedge_app_reqs}
We now discuss the requirements that \oneedge{} allows application developers to specify which allow the application
\begin{figure}
	\centering
	\includegraphics[width=0.95\textwidth]{figures/oneedge/pipeline_latency.pdf}
    \caption{
    A generic pipeline that explains the different components of the tolerable latency staleness. $S_{(i-1,i)}$ is the acceptable latency starting at the output of the client to the input of the node $i$. It is composed of both computational latency $C_j$ and transmission latency $T_{j-1,j}$.}
	\label{fig:pipeline_lat}
\end{figure}

\subsubsection{End-to-End Processing Latency}
Application developers are able to specify the end-to-end processing latency for each component in the application pipeline model. End-to-End processing latency for an application component is the time elapsed between when a specific data-item/event is generated by the client to when it is processed by the application component. This includes the network transmission latency and processing time of all the upstream components. The end-to-end latency constraint for each application component should be satisfied for each client of the given application. The end-to-end latency for client $c$ at application component with level $l$ is defined in \cref{eq:e2e_proc_latency}, which is calculated recursively using the value for the upstream component with level $l+1$. The base case is when the level $l$ equals the level corresponding to the client itself, in which case, the end-to-end latency is equal to the processing latency on the client.
\begin{equation}
\label{eq:e2e_proc_latency}
E2E \left( c, l \right) = E2E \left(c , l+1 \right) + proc \left( \mathcal{M} \left( c, l \right) \right) + net \left(  \mathcal{M} \left( c, l+1 \right) , \mathcal{M} \left( c, l \right) \right)
\end{equation}

\subsubsection{Spatial Affinity}
Several situation-awareness applications such as the collaborative driving assistance application (\cref{fig:collab_driving_pipeline}) consist of components that are tied to a specific geographical area, and are meant to serve clients located in that geographical area only. This is meant to enable information sharing between clients that are in close spatial proximity to one another. We denote the geographical area served by an application component as its \textit{spatial context}. The partitioning of the geographical space into spatial contexts is application-specific. More precisely, the application developer would specify a function $\mathcal{S}$ that maps a geographical location $\left( x , y \right)$ to a unique spatial context $s_z$ (identified by a positive integer $z$) (\cref{eq:spatial_context}).

\begin{equation}
\label{eq:spatial_context}
   \mathcal{S}: X \times Y \rightarrow \{ s_0, s_1, \cdots \}
\end{equation}
\begin{equation}
   X = \{x \in \mathbb{R}~|~-\pi < x < \pi\}
\end{equation}
\begin{equation}
   Y = \{y \in \mathbb{R}~|~\dfrac{-\pi}{2} < y < \dfrac{\pi}{2}\}
\end{equation}

For an application component of level $l$ that needs to facilitate inter-client information sharing, the control-plane assigns each spatial context $s_z$ with an application component and map all clients located within that spatial context to that instance. Multiple spatial contexts can also be mapped to the same application instance. However, the main requirement is that all clients within given spatial context should be served by the same application instance so that they can effectively share state with other clients in the same spatial context. The quality of this client-application-instance mapping is quantified using the Spatial Alignment metric, as shown in \cref{eq:spatial_alignment}. The Spatial Alignment metric is defined for each spatial context $s_z$ that is served by the application component instance $a$ at level $l$.  Ideally, the spatial alignment metric should be equal to 1 for all spatial contexts.
\begin{equation}
\label{eq:spatial_alignment}
SA \left( a \right) = \dfrac{|\{ c \in \clientset : c.loc \in s_z   \mathcal{M} \left( c, l\right) = a \}|}{|\{ c \in \clientset : c.loc \in s_z \}|}
\end{equation}
\todo{Can we connect application instance $a$ with $s_z$?}

\subsection{Workload Dynamism}

\subsubsection{Client Mobility}
Typical situation-awareness applications have clients that are inherently mobile, for example, vehicles, pedestrians, UAVs, etc. The mobility of clients creates two main challenges. For applications involving inter-client information sharing, the mapping of a client to an instance of a given component is based on the spatial context of that component's instance and the location of the client. Due to client mobility, the client's location might change so much that it exits the spatial context of the current application component instance, and thus is not able to coordinate with the correct subset of other clients that are in its physical proximity. This requires that the client be migrated to the application component instance that has the correct spatial context which corresponds to the current location of the client. Secondly, client mobility can result in a change in the network routing and hence communication latency to and from the application instance. This change in communication latency affects the end-to-end processing latency for that client's data by the current application instance. This necessitates that the client be migrated to an application instance that can satisfy the end-to-end latency requirements.

\subsubsection{Changes in Processing Requirements of Applications}
The frequency of events generated by different application components in an application pipeline changes over time. This is either due to the mobility of clients which changes the properties of the environment sensed by the clients. For instance, in the Collaborative Driving Assistance application, the number of neighboring vehicles output by the Detection component (in the client) is a function of the density of neighboring traffic, which changes with time as the ego vehicle moves. The change in processing requirements of applications can also happen for static sensors, such as CCTV cameras, when the environment they are sensing undergoes changes. For instance, in the \todo{} application, the frequency of events generated by the \todo{} component depends on the number of cars in the field-of-view of a camera, which changes over time.

\section{Architectural Components of \oneedge{}}
Now we present the system architecture of \oneedge{}, covering the design choices and deployment characteristics of each component of the system. We will first provide a high-level overview of the system, with an emphasis on describing how \oneedge{} operates and the typical operations of each component. Then we will discuss the functions of each component in more detail.

\subsection{High-level System Architecture}
\begin{figure*}[t]
\centering
\includegraphics[width=0.85\columnwidth]{figures/oneedge/onefog_overview.png}
\caption{\oneedge{}'s System Architecture. A central controller (left blow-up) coordinates with all the Edge sites (right blow-up).}
\label{fig:system_arch}
\vspace{-2mm}
\end{figure*}
\cref{fig:system_arch} shows the high-level system architecture of \oneedge{}, which comprises of two main top-level entities: the \emph{Edge Site} and \emph{Controller}. An Edge Site is an independent entity with compute and storage resources that is able to run instances of application components and serve clients. The Controller is a logically-centralized entity which is responsible for the scheduling and deployment of applications over multiple Edge Sites. 

\todo{Autonomous updates to resource allocation on sites.}

\par Application clients run the client component of the application pipeline and are served by an application instance running on one or more Edge sites. To connect to a suitable application instance, clients send deployment requests to the geographically closest Edge site (determined by using a standard discovery service\cite{citation_21_in_oneedge_paper}). If the nature of the application is such that it does not require information from other clients and the Edge site receiving the deployment request has enough resources, then the application is deployed entirely on that Edge site. Otherwise, the deployment request is sent to the Controller, which computes the application scheduling decision and launches application components across potentially multiple Edge sites.
\todo{Add that the Monitoring Subsystem issues reconfiguration requests that need to be serviced either by the controller or the site }

\subsection{Client Library}

\subsection{Components on Edge Site}
An Edge site consists of four main components, as shown in the right blow-up in \cref{fig:system_arch} - the Site Agent, Container Runtime, Container Agent and the Monitoring Subsystem. 
\subsubsection{Site Agent}
The Site Agent component is responsible for managing the resources on the site, along with the application component instances running on the site. It receives requests from clients for connection to an application instances, and deploys instances of the application pipeline's components in response. It also interfaces with the central Controller for the centralized scheduling of applications on that Edge site. Specifically, the Site Agent provides periodic updates to the Controller about the allocation state of application component instances running on the site. In addition, it also receives commands from the Controller to deploy application component instances. The Site Manager is also responsible for lifecycle management of the running application component instances, wherein, it updates the resource allocation of these instances based on requests triggered by the Monitoring Subsystem, which will be discussed in more detail in \cref{sec:oneedge_monitoring}.
\subsubsection{Container Runtime}
The Container Runtime is the software platform upon which the various application component instances run as containers. The container runtime provides primitives for launching containers based on an application-specific image, allocation a specific amount of compute and memory resources to containers to ensure predictable performance and isolation, access to a filesystem for storing ephemeral state and communication with other entities in the system. 
\subsubsection{Container Agent}
The Container Agent is a software agent which is deployed alongside the application logic within each application component instance's container. It acts as the interface of the application logic for that specific application component with the rest of the components and the outside world. It does so by implementing the various interfaces provided to the application developer. The container agent facilitates communication between various application instances by allowing upstream and downstream components to send messages to each other, as well as trigger callback functions in the application logic upon message arrival. Furthermore, it also serves as a part of the monitoring subsystem, whereby it collects metrics related to the execution of the application component and forwards them for further processing.
\subsubsection{Monitoring Subsystem}
\todo{Add text for the Monitoring Subsystem.}

\subsection{Controller}
The Controller component is responsible for application scheduling and management at a global-scale across Edge sites. It receives requests for application deployment and reconfiguration from clients and the monitoring subsystem respectively. These requests are processed by the Scheduler, which turns these requests into a transaction (set of actions) to be executed on one or more Edge sites. The transaction is then added to the pool of pending transactions waiting to be executed on the respective Edge sites. The Transaction Executor picks up a pending transaction and executes the actions contained in them on one or more Edge sites. We will discuss each of the components in greater detail next.
\subsubsection{Scheduler}
\label{sec:scheduler}
\begin{itemize}
\item The Scheduler picks up one request at a time from the Controller's request queue and computes a scheduling decision for the request. The scheduling decision is computed by executing the placement algorithm (\cref{todo}) for mapping the requesting client to a suitable application instance that can satisfy the end-to-end latency and spatial affinity requirements. If no such application instance exists, a new instance is instantiated and suitable Edge sites for hosting the application components are selected.
\item In the case of a reconfiguration request for updating allocation of an application instance, the scheduler computes the final resource allocation for the components of that application instance.
\item The Scheduler reads the Aggregate Resource State to check the current available resource capacity on each Edge site, and updates the state with changes to the resource allocation. Hence, the Aggregate State is \textit{optimistically} updated even before the scheduling decision has actually been applied on the specific Edge sites. By doing so, the process of compute application scheduling decisions is decoupled from the actual enforcement of those decisions on the Edge sites.
\item The scheduling decision is in the form of a Transaction, which is a collection of the actions that need to be taken to execute the scheduling decision. The constituent actions of a transaction need to executed on one or more Edge sites. 
\end{itemize}

\begin{minted}{yaml}
-   app_id: 
    txn_id: 
    actions: 
        -   site_id: 
            site_actions:
                -   level: 
                    type: DEPLOY
                    resources: 
                        cpu:
                        memory: 
                -   level: 
                    type: MODIFY_ALLOC
                    resources:
                        cpu: 
                        memory:
\end{minted}

\subsubsection{Aggregate State}
\label{sec:aggr_state}
\begin{itemize}
\item As described in \cref{sec:scheduler}, the Aggregate State is used by the Scheduler to make application scheduling decisions. For subsequent requests' processing to be aware of the decisions made for the current request, the Aggregate State is updated with the current request's decisions. 
\item The Aggregate State maintains an optimistic version of the resource allocation state, assuming that all the scheduling decisions taken by the Scheduler will successfully get enforced on the Edge sites.
\item However, since Site Agents have the autonomy to make resource allocation changes without coordinating with the Controller, scheduling decisions from the Controller could be rejected by the Site Agent, in which case the corresponding Transaction would be aborted.
\item The abortion of a transaction would result in rolling back the Aggregate State to a state before the aborted transaction's processing was begun by the Scheduler.
\item In addition to having applied updates that have not yet been successfully applied on Edge sites, the Aggregate State is also eventually consistent with the ground-truth resource allocation which is maintained by the Site Agents. Since Site Agents are able to update allocation state of the site without coordinating with the Controller, the Controller's Aggregate State does not see those autonomous updates until the state is synchronized with the Edge site, which is done periodically.
\end{itemize}
\subsubsection{Pending Transactions Pool}
The Transactions generated by the Scheduler are buffered in the Pending Transactions Pool so that their constituent actions can be enforced on the Edge sites. However, transactions can only be executed in the order in which they were processed by the Scheduler. The relationship between transactions that defines the order in which they are executed can be denoted as $T_i \leftarrow T_j$, in this case denoting that the transaction $T_i$ can be executed only when $T_j$ is executed. In other words, the Aggregate State output by $T_j$ was used as input for $T_i$. The following conditions have to be met for $T_i \leftarrow T_j$ to be true:
\begin{itemize}
\item $T_j$ was generated by the Scheduler before $T_i$.
\item The set of Edge sites affected by $T_i$ and $T_j$ overlap.
\end{itemize}

\subsubsection{Transaction Executor}
\begin{itemize}
\item 
\end{itemize}
\section{Spatial-Affinity-aware Application Scheduling using Dynamic Spatial Context Management}
\begin{itemize}
\item An application can have multiple components that are tied to their specific spatial contexts. The constraint imposed by \oneedge{} is that the spatial context of an upstream component should be completely present inside the spatial context of the downstream component. This is done to ensure that the each instance of the downstream component has only one parent instance. 

\end{itemize}
\subsection{Flexible Partitioning of Geographical Space}
\begin{itemize}
\item For each of the application components that are spatially constrained, the control-plane maintains a KD-Tree based spatial partitioning, as discussed in \cref{sec:spatial_ctx_mgmt}. 
\end{itemize}

\subsection{Control-Plane Policy for Spatial-Affinity-aware Application Scheduling}

\section{Latency-aware Application Scheduling using Network Proximity Estimation}

\subsection{Deployment of Network Coordinate Agents}

\subsection{Estimating End-to-End Processing Latency}

\subsection{Control-Plane Policy for Latency-aware Application Scheduling}

\section{Distributed Monitoring in \oneedge{}}
\begin{itemize}
\item The goal of monitoring in \oneedge{} is to detect violations of performance SLO, which in this case is end-to-end processing latency.
\item The end-to-end processing latency constraint is specified for one or more components of the application pipeline. The end-processing-latency at a certain component is the sum of the processing and communication latencies of the application pipeline instance right from the client up to (and including) the given component.
\item The objective of \oneedge{} is to detect when any constraint on end-to-end processing latency in an application instance is violated, and then trigger the appropriate reconfiguration action to alleviate that violation.
\item Another crucial objective of monitoring in \oneedge{} is to identify the root-cause of the end-to-end processing latency violation, which is used to determine the appropriate reconfiguration action that can alleviate the violation.
\end{itemize}

\subsection{Metrics}
Each application component instance registers two metrics with the monitoring subsystem. 
\begin{enumerate}
\item The network latency between the given application component instance and its immediate downstream component (also known as \textit{parent}). 
\todo{Add parent ping process in ContainerAgent description}
\begin{minted}{yaml}
-   entity_id : <ID of application component instance>
    level : <level in application pipeline of component>
    app_unit: <ID of root component instance in application tree>
    metric: "net_latency"
    parent: <ID of parent component instance>
\end{minted}
\item Processing latency at the given application component instance.
\begin{minted}{yaml}
-   entity_id : <ID of application component instance>
    level : <level in application pipeline of component>
    app_unit: <ID of root component instance in application tree>
    metric: "proc_latency"
    parent: <ID of parent component instance>
\end{minted}
\todo{Do we need parent field in $proc\_latency?$}
\end{enumerate}

\subsection{Violation Detection Policy}
\begin{algorithm}
\caption{$End-to-End Processing Latency Violation Detection policy$}
\begin{algorithmic}
\Require ID of root worker $ID_{root}$ ($A_{00}$ in \cref{fig:pipeline_example}).
\Require Application abstract $APP$
\State $MD \gets \text{Fetch metrics where AppUnit == }A_{00}$
\State $C \gets \{ m.entity\_id \forall m \in MD : m.level == APP.num\_levels-1 \}$
\State $entities \gets \{\}$ \Comment{Dictionary to hold proc and net latencies for each worker}
\For{$m \in MD$}
    \If{$m \notin entities$}
        \State $entities[m.entity\_id] \gets \{\}$
    \EndIf
    \State $entities[m.entity\_id][m.metric] \gets m$ 
\EndFor

\State $violators \gets \{\}$
\For{$c \in clients$}
    \State $e \gets c$  
    \State $e2e \gets 0$
    \State $level \gets APP.num\_levels-1$
    \While{$level \geq 0$} \Comment{Iterating over all levels of app pipeline}
        \State $e2e += entities[e][proc\_latency].val + entities[e][net\_latency].val$
        \If{$e2e \geq APP.e2e\_threshold[level]$}
            \State $violators \gets violators \cup \{ \left( c, level \right) \}$
        \EndIf
        \State $level -= 1$
        \State $e \gets entities[e][net\_latency].parent$
    \EndWhile
\EndFor
\Return $violators$
\end{algorithmic}
\end{algorithm}

\subsection{Violation Root-Cause Analysis Policy}

\section{Implementation}

\section{Evaluations}

\section{Conclusion}