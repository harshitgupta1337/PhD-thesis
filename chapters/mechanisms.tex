\chapter{Necessary Mechanisms for Geo-Distributed Operation of Platform Services}
\begin{comment}
How to realize platform services at the edge?
    - describe core mechanisms needed
    - discuss how these mechanisms will enable efficient implementation of the aforementioned platform services in geo-distributed edge infrastructure
    - discuss the conceptual relationship of these mechanisms to the state-of-the-art in the cloud and other related research works
\end{comment}

\section{Introduction}
The challenges imposed by the peculiar characteristics of a geo-distributed Edge infrastructure on the design of  control-plane for platform services necessitate the introduction of novel mechanisms to address them. We propose three key mechanisms in this dissertation to address these challenges - Dynamic Spatial Context Management, Network Proximity Estimation and End-to-End Application Monitoring. The Dynamic Spatial Context Management mechanism allows the control-plane of platform services to maintain a frequently updated view of the spatial context of system entities such as application instances, data-items and end-clients. This spatial context information is used to make control-plane policy decisions, such as mapping a client to an application instance, determining the set of clients that need to share data for inter-client coordination, etc. The objectives of these control-plane policies is to ensure that the spatial affinity requirement of an application is fulfilled. While the Dynamic Spatial Context Management mechanism is used to establish logical connectivity between system entities based on spatial proximity, it is not responsible for mapping those system entities on to the physical infrastructure - a problem that requires infrastructure topology heterogeneity and timing considerations of applications to be kept in mind. 
The Network Proximity Estimation mechanism allows control-plane policies to estimate the network latency between physical nodes in the infrastructure which can be used for the placement of system entities such that applications' requirements are satisfied. Finally, the continuous execution of applications requires the control-planes of platform services to monitor the end-to-end latency experienced by each application instance, which includes both computation delays and communication delays of data between different entities. The End-to-End Monitoring mechanisms allows the control-plane policies to obtain a time-aligned and aggregated view of the various component latencies making up the end-to-end observed latency so that the policies can detect a performance violation and perform a root-cause analysis to identify the source of the performance violation and trigger the appropriate reconfiguration action to alleviate it. 
\par In this chapter, we will discuss the three mechanisms proposed in this dissertation in detail. For each mechanism, we will first describe the objective of the mechanism, enumerate examples of control-plane policies that will benefit from it, present the abstractions exposed to the control-plane policies, and why previous approaches at attaining this objective are not sufficient. Finally, we demonstrate the use of these abstractions for building useful control-plane policies that can support the efficient operation of platform services on a realistic Edge infrastructure against a workload of situation-awareness applications. Before we delve into the specifics of each mechanism, we present the scenario that has been considered to motivate the problems solved by these mechanisms and to design the experimental settings. 

\subsection{Infrastructure Topology considered}
\label{sec:nep_infra_topology}
\par To make the case for the mechanisms proposed in this dissertation, we utilize the dataset released by a previous work \cite{xu2021cloud}. The dataset characterizes the Edge cloud service of Alibaba Cloud, both at the infrastructure and workload level. At the infrastructure level, it provides detailed information about the number of edge sites in each city and the network provider who owns them, the number and size of physical machines in each edge site and the network round-trip time between sites. At the workload level, the dataset contains information about the number and size of VMs hosted by each physical machine and the CPU utilization of each VM. For this evaluation, we focus on the city of Shanghai. We simulate client activity (including mobility) in the city of Shanghai, and thus to determine the network connectivity between a client and Edge site, knowing the geographical coverage of each Edge site is necessary. To estimate the coverage of each Edge site, it is necessary to know the precise geographical location of Edge sites and their connectivity with the network access points, or cell towers. However, the dataset only provides a city-level granularity of Edge site locations. Hence, to approximate their precise geo-location, we gather the locations of cell towers owned by the different network providers from CellMapper \cite{cellmapper}, perform k-means clustering on them and obtain the likely location of the edge sites. The number of k-means clusters for each network provider's cell tower clustering is made equal to the number of Edge sites owned by that provider in Shanghai (from the dataset). Having obtained the locations of Edge sites using clustering, we need to In addition to providing the number of Edge sites and their network providers in each city, the Alibaba Edge Node Service dataset also contains information about the resource capacity of each Edge site. Hence, to map the resource capacity from the dataset to an Edge site location extracted via clustering, we use an approach that assigns the most resource-rich site's resources in the dataset to the Edge site location that has the maximum number of cell towers in its cluster, and so on. \cref{fig:shanghai_infra} illustrates the infrastructure topology, with the locations of cell towers and Edge sites marked. In addition to Edge site within Shanghai with fine-grained locations, the physical infrastructure considered also consists of Edge sites in other cities of China, wherein their location is set to the center of that city, because they are significantly farther away from any client in Shanghai compared to any site within Shanghai. 

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/mechanisms/shanghai_infrastructure.JPG}
\caption{Edge infrastructure of the city of Shanghai which is under consideration in this chapter. The crosses denote Edge site locations, while the dots denote cell towers. \todo{Add lat-lng to the axes labels. Update the legend to be more meaningful.}}
\label{fig:shanghai_infra}
\end{figure}

\section{Dynamic Spatial Management Mechanism}
\label{sec:spatial_ctx_mgmt}

Situation-awareness applications, e.g., collaborative autonomous driving and UAV swarm navigation, interact with the physical environment, by sensing environment data and performing actions on it. Typically, there are many clients of the same application operating in a common geographical space. In such a setting, a client's processing logic can benefit by incorporating information extracted from other clients' sensed data. For a given client, the set of other clients that it needs to coordinate with depends on the location of clients, the size of their sensor range and the area of interest of the given client. The area of interest (AoI) of a client is defined as the geographical region about which it is interested in receiving information. The sensing range of a client depends on the sensor hardware used, e.g., LiDAR or camera. The size of the Area-of-Interest depends on the nature of the application. For instance, since vehicles in a city are not expected to move at speeds higher than , say, 25 miles per hour (in urban cities in USA), the size of AoI of vehicular clients for the collaborative perception application is bounded. There exists only a finite region around a given vehicle that would contain any interesting event for the vehicle.
\begin{figure}
\centering
\includegraphics[width=0.75\textwidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_range.pdf}
\caption{An exemplary spatial distribution of situation-awareness application clients. The sensing range of clients is shown as a red dashed rectangle around the client's location. The Area-of-Interest of client C1 is shown.}
\label{fig:aoi_range}
\end{figure}
\cref{fig:aoi_range} shows a typical arrangement of clients in geographical space with the sensing range of each client marked in red and the AoI of one of the clients C1 marked in blue. Each client senses data from the environment within its range and generates processed information about it. Client C1 is interested in receiving all information about the subset of environment within its AoI's bounding-box. Hence, the set of clients that it needs to interact with are those whose range's bounding-box overlaps with the AoI bounding-box of C1. 

\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_range_partition.pdf}
  \label{fig:aoi_range_partition}
  \caption{The partitioning of geographical space. Each partition is to be managed by a distinct application component. The important point to note is that each partition is not self-sufficient, but rather relies on information from other partitions as well. This is because the AoI of clients inside a given partition overlaps with the range of clients in other partitions.}
\end{subfigure}%
~~~
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_range_partition_mapping.pdf}
  \caption{Clients in each spatial partition are mapped to a unique region-level component that is responsible for fusing the information extracted from each individual client.}
  \label{fig:aoi_range_partition_mapping}
\end{subfigure}
\caption{Illustration of partitioning of geographical space to support large-scale deployment of situation-awareness applications that require coordination among clients.}
\label{fig:spatial_partitioning}
\end{figure}

An intuitive way of modeling these applications was discussed in \cref{sec:app_model_compute}, wherein a region-level component is responsible for fusing the information extracted from multiple clients to realize inter-client coordination. However, in a large-scale geo-distributed deployment of such applications, a single region-level component would be insufficient to serve all the clients - because of scalability limitations of the fusion application component, resource constraints on the Edge site hosting it, or both \cite{talkycars}. Hence, to support a large number of clients, multiple instances of region-level component are maintained as shown in \cref{fig:spatial_partitioning}, with each instance serving a distinct partition of the entire geographical area. All clients within a given spatial partition should be served by the same region-level instance, thus enabling inter-client coordination among all clients within that partition. Due to the inherent mobility of clients and the fact that the spatial partitioning is not necessarily aligned with the range of each client, a client $i$'s AoI can overlap with another client $j$'s range that is located in a different partition. To make sure that information from client $j$ is taken into account while processing the sensor data of client $i$, the region-level component instance serving client $i$ receives a stream of fused information from the instance serving client $j$, as shown in \cref{fig:aoi_range_partition_mapping}. 

\par For situation-awareness applications that require inter-client coordination, mapping clients to region-level component instances needs to be done by taking into account the spatial context of clients (denoted by their location) and that of the region-level instances (denoted by spatial partition they are meant to serve). Information sharing between region-level instances is also dictated by the spatial context of the different instances and the range and AoI of clients served by those instances, as shown in \cref{fig:spatial_data_sharing}. 
\begin{figure}
\centering
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/out_data.pdf}
  \label{fig:spatial_ctx_out_data}
  \caption{An illustration of the information generated by a given region-level component instance that needs to be shared with other instances.}
\end{subfigure}%
~~~
\begin{subfigure}{0.4\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/in_data.pdf}
  \caption{An illustration of the information needed by a region-level application component from other application instances. }
  \label{fig:spatial_ctx_in_data}
\end{subfigure}
\caption{The shaded regions in both images show the data which needs to be shared with other region-level instances (\cref{fig:spatial_ctx_out_data}) and which needs to be received from other instances (\cref{fig:spatial_ctx_in_data}). The $R$ denotes the size of sensing range, while $A$ represents the size of AoI of each client.}
\label{fig:spatial_data_sharing}
\end{figure}
Thus, to ensure that each client is served by an application instance specific to its spatial context and that application instances are able to share relevant data among each other, it is imperative to treat the spatial context of clients, application components and data-items as first-class attributes. The main challenge is to handle continuous client mobility, which results in the AoI and spatial context of each client to change continuously. Hence, a static mapping of clients to application instances would result in frequent violation of spatial affinity. Furthermore, given the continuous mobility of clients and the limited resource capacity of edge resources, workload skews are much more common - caused due to a large number of clients accumulating in a particular partitition. Such spatial workload skews would necessitate the monitoring and remapping of geographical regions to applications instances and data-items, so that the skews can be minimized and performance hotspots can be avoided.

\subsection{Control-plane policies that need this mechanism}
\par \textbf{Dynamic Client-Application Mapping. } Situation-awareness applications require that a client be connected to an application component that is assigned to the geographical area within which the client is currently located. A metric that quantifies the goodness of this mapping is Spatial Alignment, which measures how many of the expected clients that should have been mapped to an application component are actually mapped. The metric for spatial partition $A$ is quantified in \cref{eq:spatial_alignment}.
\begin{equation}
SA \left( A \right) = \dfrac{\text{max. clients in }A\text{ served by the same app instance}}{\text{number of clients in }A}
\label{eq:spatial_alignment}
\end{equation}
The control-plane policy for client-application mapping would ideally ensure the spatial alignment metric for all spatial partitions is 1.0, meaning that all clients that are currently located in a given spatial area A are connected to the same application instance.

\par \textbf{Area-of-Interest Queries. } Clients and application components need to query system entities whose spatial context overlaps with the querying entity's AoI. This query is served by the control-plane which evaluates the spatial context overlap with querying entity's AoI and returns a list of satisfying system entities. We use a metric called AoI Satisfaction Rate to quantify the goodness of the query, as shown in \cref{eq:aoi_sat_rate}.
\begin{equation}
\text{AoI Satisfaction Rate} = \dfrac{|\{ e: e \text{ is returned by query  and } e \in AoI \}|}{|\{ e: e \in AoI \}|}
\label{eq:aoi_sat_rate}
\end{equation}
Ideally the control-plane policy for evaluating these queries should be able to achieve an AoI Satisfaction Rate of 1.0.


\subsection{Limitations of previous work in implementing policies}
Contemporary Edge computing solutions don't allow coordination among application components that are deployed across multiple Edge sites \cite{gabriel, azure_iot_edge}. Hence, a client $c$ that is mapped to an Edge site $E$ can only coordinate with other clients that are also mapped to the site $E$. Similarly, $c$ can only discover other system entities that are connected to the site $E$. We consider 2 high-level approaches in which clients have been mapped to Edge sites in previous work - mapping clients to the geographically closest site (GeoDist) (as in \cite{lahderanta2021edge}) and to the site with smallest RTT from the client (RTT) (as in \cite{foglets}). We show how both these approaches are deficient in satisfying the spatial alignment requirement of client-to-application mapping as well as serving Area-of-Interest based queries.

\begin{figure}
\centering
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/spatial_alignment_randomized_4_rows.png}
  \caption{}
\end{subfigure}%
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/spatial_alignment_randomized_8_rows.png}
  \caption{}
\end{subfigure}\par\medskip
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/spatial_alignment_randomized_16_rows.png}
  \caption{}
\end{subfigure}%
\begin{subfigure}{0.45\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/spatial_alignment_randomized_32_rows.png}
  \caption{}
\end{subfigure}
\caption{Distribution of Average Spatial Alignment observed for different sizes of spatial partitioning. Closest RTT based mapping of clients to Edge sites results in worse spatial alignment compared to Closest Geo Distance based mapping.}
\label{fig:spatial_alignment_eval}
\end{figure}

\par \noindent \textbf{Spatial Alignment Evaluation. }To evaluate the above client-to-application mapping baseline heuristics in terms of spatial alignment, we first consider the area of the city under evaluation and divide it into a number of spatial partitions, within which we ideally expect clients to be grouped and served by the same application instance (thereby creating a perfect spatial alignment). The size of spatial partitions is varied in the experiment to represent a diverse set of applications. We create 1000 clients (with equal number of clients in each network provider) and place each one of them at a cell tower location. The placement of clients at cell tower locations is justified by the fact that the spatial distribution of cell towers follows that of client activity. This client placement is randomized and the experiment is repeated 100 times. For each experiment run, we compute the average spatial alignment over all spatial partitions in the scenario. \cref{fig:spatial_alignment_eval} shows the distribution of the average spatial alignment that results from mapping clients to application instances using greedy heuristics that aim at minimizing geographical distance and network RTT between the client and Edge site. The metric shown is average spatial alignment, which is the average of the spatial alignment of all the spatial areas. Although the baseline policy GeoDist, which selects the geographically closest Edge site, is able to attain a high enough average spatial alignment, but doesn't attain the perfect 1.0 value because not all clients in a given spatial partition always have the same Edge site to be the closest. Furthermore, the Closest RTT approach offers even worse spatial alignment because in a given spatial partition, clients belonging to two different network providers are bound to be mapped to different sites. 

\begin{figure}
\centering
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_satisfaction_rate_cdf_AOI_0.100_km.png}
  \caption{}
\end{subfigure}%
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_satisfaction_rate_cdf_AOI_0.200_km.png}
  \caption{}
\end{subfigure}
\begin{subfigure}{0.3\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_satisfaction_rate_cdf_AOI_0.400_km.png}
  \caption{}
\end{subfigure}
%\par\medskip
\begin{subfigure}{0.333\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_satisfaction_rate_cdf_AOI_0.800_km.png}
  \caption{}
\end{subfigure}%
\begin{subfigure}{0.333\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_satisfaction_rate_cdf_AOI_1.600_km.png}
  \caption{}
\end{subfigure}%
\begin{subfigure}{0.333\textwidth}
  \centering
  \includegraphics[width=\linewidth]{figures/mechanisms/spatial_ctx_mgmt/aoi_satisfaction_rate_cdf_AOI_3.200_km.png}
  \caption{}
\end{subfigure}
\caption{AoI Satisfaction Rate}
\label{fig:aoi_satisfaction_rate_eval}
\end{figure}

\par \noindent \textbf{AoI Satisfaction Evaluation.} Next, \cref{fig:aoi_satisfaction_rate_eval} shows the distribution of AoI query satisfaction rate offered by the Closest GeoDist and Closest RTT baseline policies for retrieving the system entities with spatial context overlapping with the AoI of the querying client. For this evaluation, create 1 client at every cell tower location, and each client submits an AoI query to find which other clients are in its AoI. Depending on the client-Edge-site mapping, a variable number of AoI members are returned, which is compared with the actual set of clients within the AoI to compute the query satisfaction rate. The size of AoI query is varied to represent a diverse set of applications and their satisfaction rate distributions are shown. Both client-to-application mapping approaches perform poorly and fall significantly short of the expected metric values.

\subsection{Interface exposed to the control-plane}
The Dynamic Spatial Context Management mechanism is utilized by both the centralized control-plane compute/data placement policy of platform services as well as the client library of the platform service running on the client application. The centralized control policy interacts with this mechanism to maintain the spatial context of the system entities (data and compute) while the client library uses this mechanism to keep track of the spatial context of clients. Both these types of spatial contexts are useful in determining which compute/data entities a given client would need to access. 
\par We divide the geographical space into rectangular regions called \textit{Tiles}, which are of arbitrary size. An example partitioning is shown in \cref{fig:aoi_range_partition}. Each Tile has a unique Tile ID, which can be used to directly reference the Tile. Each tile contains a number of entities, such as clients or data-items. In the following list, we enumerate the various functions that the mechanism exposes to the control-plane for accessing and updating this partitioning.
\begin{itemize}
\item \textbf{GetTileID}. This function takes a location as input and returns the ID of the Tile inside which that location falls.
\item \textbf{GetTileCoverage}. This function takes the ID of a tile as input and returns the bounding box which represents the spatial area covered by the tile.
\item \textbf{UpdateEntityLocation}. Update the location of an entity. In addition, if the new location of the entity is in a different tile than its previous location, this function would move the entity from previous tile to the new tile. 
\item \textbf{GetIntersectingTiles}. This function takes in a bounding box and returns the set of tiles that intersect with it. This function is useful for range queries.
\item \textbf{SplitTile}. This function call is used to split a tile into two. The split is carried out in a way to ensure that the number of entities in the two resultant tiles is (almost) equal. This function is used to handle load.
\item \textbf{MergeTiles}. This function allows the control-plane to trigger a merging of 2 sibling tiles.
\end{itemize}

\subsection{Demonstration of using the mechanism for implementing a control-plane policy}
We now demonstrate how the proposed mechanism is useful in implementing control-plane policies for platform services. For this mechanism, we use an application orchestrator as the driving example. \cref{algo:deploy_req} shows the pseudocode of the control-plane policy for finding a suitable application instance for mapping a client based on its geographical location and the spatial contexts of various region-level application instances. The control-plane maintains a mapping $APPS$ between an application instance and the Tile which represents the geographical area served by the application instance. The policy first determines the Tile in which the client is currently located. If the client's tile already has an application instance associated with it, it returns connection information about that instance. Otherwise it deploys a new application instance for the client's tile, maps the application instance to the tile and returns its information to the client. The policy also adds the client to the tile to update the occupancy information, which can be used to detect overload and trigger re-partitioning.
\begin{algorithm}
\caption{Handling Deploy Request from Client}
\begin{algorithmic}
\Require client $c$
\Require client's location $loc$
\State $t \gets GetTileID \left( loc \right)$
\If{$APPS \left[ t \right] exists$}
    \State $A \gets APPS \left[ t \right]$
\Else
    \State Deploy app component $A$ for tile $t$
    \State $APPS  \left[ t \right] \gets A$
\EndIf
\State $UpdateEntityLocation \left(t, c \right)$
\State $SendConnectionInfo \left(c, A \right)$ \Comment{Send info to client for connecting to app component}
\end{algorithmic}
\label{algo:deploy_req}
\end{algorithm}

\cref{algo:handle_overloaded} describes the control-plane policy for handling an overloaded region-level component due to workload skew. This policy is triggered by the control-plane when it identifies that a given application instance is overloaded by the compute requirement of serving all the clients that are currently present in the tile its is serving. Hence, the tile needs to be split and workload divided among two application instances. The policy takes as input the tile associated with the overloaded application instance. The policy uses the \textit{SplitTile} function to create two new tiles. It reuses the application instance for the old tile for one of the new ones, and deploys another instance for the second new tile. Each client that belonged to the old tile is now a part of one of the two new tiles. The policy sends the connectivity information of the new application instances to the respective clients. 
\begin{algorithm}
\caption{Handling an Overloaded Tile}
\begin{algorithmic}
\Require overloaded tile $t$
\State $t1, t2 \gets SplitTile \left( t \right)$
\State $APPS \left[ t1 \right] \gets APPS \left[ t \right]$
\State Deploy app component $A$ for tile $t$
\State $APPS  \left[ t2 \right] \gets A$
\For{client $c \in GetEntities \left( t1 \right)$}
    \State $SendConnectionInfo \left(c, APPS \left[ t1 \right] \right)$
\EndFor
\For{client $c \in GetEntities \left( t2 \right)$}
    \State $SendConnectionInfo \left(c, APPS \left[ t2 \right] \right)$
\EndFor
\end{algorithmic}
\label{algo:handle_overloaded}
\end{algorithm}

\section{Network Proximity Estimation}
The performance perceived by applications when using platform services heavily depends on the mapping of compute and data components on to the underlying physical infrastructure \cite{sarkar2016theoretical,amarasinghe2018data,naas2017ifogstor,liu2019mobility}. In a densely geo-distributed and heterogeneous infrastructure such as the Edge, the network latency between a client and Edge site varies significantly depending on which edge site the client is communicating with. Similarly, the network latency between edge sites is also highly heterogeneous.
Hence the choice of Edge Sites chosen to host compute or data components of an application instance has a significant effect on the perceived end-to-end latency of the application, be it the sense-process-actuate control-loop's latency or the latency of accessing data from other clients for inter-client coordination. Therefore, placement policies in the control-plane of platform services need to be aware of the topology of the underlying infrastructure to make their placement decisions. For instance, in the case of the collaborative perception application, the placement of application components should be such that the end-to-end processing latency is bounded under the application's  threshold. Similarly, in the case of the drone swarm coordination application, the publish-subscribe topic should be placed on a suitable broker node to ensure that the end-to-end message delivery latency is bounded under the application's threshold.  
\par Therefore, the platform services require a mechanism that enables their control-plane to estimate the network latency between the end-clients and Edge Sites as well as across Edge Sites. Such a mechanism would be able to estimate the network latency between a pair of entities quickly and with low overhead, making it suitable to be used as a part of the placement policies of these platforms which evaluate a number of Edge Sites as candidates for compute or data placement. Given the dynamic nature of the Edge infrastructure and client mobility, the network proximity estimation should also be able to adapt to dynamic changes in network latency between system entities.

\subsection{Control-plane policies that need this mechanism}
Network proximity estimation is useful by control-plane policies that map logical system entities, such as application instances or publish-subscribe topics, to physical nodes in a way that the end-to-end latency requirements of applications are met. Previous work in this space \cite{amarasinghe2018data,naas2017ifogstor,liu2019mobility} relies on inter-node communication latency estimates for making these decisions. However does not go into detail about how these estimates are obtained, assuming instead that they are readily available for use by the policy. For the application orchestrator platform service, where typical situation-awareness applications  consist of a pipeline of multiple components \cite{ananthanarayanan2017real,das2018edgebench}, the end-to-end latency is the sum of the processing latency at each application component and the communication latency between each upstream-downstream pair of components. Similarly, in the case of a publish-subscribe system, the end-to-end messaging latency for a given topic is the sum of the communication latency from the publisher clients to the broker, the processing latency on the broker and the communication latency from the broker to the subscriber clients. In the above two systems, estimating the processing latency of application components and publish-subscribe broker can be done using previous works in the cloud computing realm as well \cite{khare2018scalable}. However, estimating the communication latency between a pair of nodes requires the proposed mechanism. Hence, the application placement policy for the orchestrator and topic placement policy for publish-subscribe system can benefit from the proposed mechanism. 

\subsection{Limitations of previous work in proximity-aware compute/data placement}
Control-plane policies for ensuring that compute and data entities accessed by a given set of clients is placed in their proximity have been one of the main directions of previous research in edge computing. The use of geographical distance as a proxy for network latency has been commonly used, given the simplicity of the scheduling logic once the geolocations of system entities (e.g., clients and edge sites) are known. Sarkar and Misra \cite{sarkar2016theoretical} propose the transformation of geographical distance between client and candidate edge site into the network latency between them using a linear transformation $rtt\left(ms\right) = dist \left(km\right) * 0.02 + 5$ \cite{qureshi2010power}. Other works don't rely on the use of such a transformation, but rather perform greedy placement on the geographically closest site to ensure low-latency access to the data or compute entity \cite{lahderanta2021edge}. Geographical location has also been used in a more coarse-grained manner, in which the placement of compute/data entities is specified to be within a large location, e.g., within a city \cite{vilaccageolocate}. The selection of the specific edge site is done on the basis of a 2nd order policy logic that aims to ensure even load distribution among the various edge sites in that particular geographical area.
\par The above approaches fail to work in realistic edge settings because they assume that geographical proximity is correlated with network proximity, which is not true because of lack of uniformity in the way in which different network providers are peered with each other. Two systems entities (a client and an edge site, or two edge sites) in close physical proximity might have to communicate through extended routing paths because of the peering between the network providers serving the two entities. In \cref{fig:geodist_vs_rtt}, we present the variation of network round-trip time between an edge site located in Shanghai to other edge sites throughout China. It shows that although network latencies are loosely correlated with geographical distance, there is a significant amount of variance, which will result in control-policies choosing an incorrect edge site for compute/data placement. In \cref{fig:same_city_diff_prov}, we specifically measure the network round-trip times between sites that are located in the same city, but are present in different network providers. The high round-trip times are reason to assume that the high variation in \cref{fig:geodist_vs_rtt} is due to the expensive peering between different network providers hosting edge sites. 
\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mechanisms/nw_proximity/shortest-rtt-vs-dist.png}
        \caption{Variation of network RTT with geographical distance between Edge Sites.}
        \label{fig:geodist_vs_rtt}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mechanisms/nw_proximity/same_city_diff_provider_rtts.png}
        \caption{Client-Site RTT for sites selected by transforming the geographical distance into network RTT and uniformly choosing one among the set of sites satisfying the latency constraint.}
        \label{fig:same_city_diff_prov}
    \end{subfigure}
    \caption{Variation of network RTT between edge sites with respect to geographical distance. \todo{Improve visibility of fonts}}
\end{figure*}

\begin{figure*}[t!]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mechanisms/nw_proximity/geodist_greedy.png}
        \caption{Client-Site RTT for sites selected by Greedy approaches. In the case that the closest site is overloaded, the next closest site is considered and so on.}
        \label{fig:geodist_greedy}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/mechanisms/nw_proximity/geodist_trans_rtt.png}
        \caption{Client-Site RTT for sites selected by transforming the geographical distance into network RTT and uniformly choosing one among the set of sites satisfying the latency constraint.}
        \label{fig:geodist_trans}
    \end{subfigure}
    \label{fig:baseline_proximity_policies}
    \caption{Performance of Edge Site selection approaches that rely on geographical distance as a proxy for network proximity.}
\end{figure*}

\par We evaluate the efficacy of the aforementioned baseline proximity estimation techniques - the greedy distance-minimization approach and the one that transforms geographical distance to network RTT. For this experiment, we consider the infrastructure topology of the city of Shanghai as described in \cref{sec:nep_infra_topology}. We simulate clients at randomly (uniformly) chosen cell tower locations and aim to find an Edge site to host its application instance using one of the above two policies. The goodness of an edge site selection is quantified by the observed RTT between the client and chosen edge site. 
\par The Greedy policy always selects the closest Edge site in terms of geographical distance between the client and the site. In case the chosen Edge site is undergoing compute overload, the policy selects the next closest site, and so on. The other site selection policy that we evaluate is one that transforms the geographical distance between client and edge site to network round-trip time and uses this estimate to filter out those sites that can meet the latency bound imposed by the application. Among the filtered sites, the policy uniformly selects one so as to minimize workload skews.
\par \cref{fig:geodist_greedy} shows the observed RTT when selecting site using the greedy approach. For more than 50 percent of the clients, the geographically closest site is not the one that it is directly connected to over the network, and hence, traffic needs to go through the network provider's core, or through the Internet to get to the selected site. Since there is no correlation between geographical distance and network RTT, this behavior is similar for 2nd and 3rd closest sites as well. For the distance-RTT transformation based approach, we compare the site selection with an application-imposed RTT bound of 10, 20 and 40 milliseconds. We see however, that the transformation does not sufficiently filter out infeasible sites and a large majority of clients have the bound violated.
\par The key takeaway from the above experiments is that using geographical distance between system components overlooks the fundamental aspect of real-world network topologies, in that the network route taken by packets seldom aligns with the geographically shortest path between the source and destination entities. Furthermore, there are additional delays caused at the various intermediate hops. Hence, any proximity estimation technique that aims at estimating network communication latency should derive its information from actual measurement of the network latency between system entities.

\subsection{Interface provided to control-plane}
All system components of the platform service (clients, worker nodes, data nodes, etc.) are assigned a unique network proximity identifier. The network proximity mechanism then provides the platform service a function \textbf{NetworkRTT} which takes as input the IDs of two system components and returns the estimated network round-trip time between the two components. 

\subsection{Using network proximity mechanism for compute/data placement}
\label{sec:policy_using_nw_prox}

We now demonstrate the use of the proposed Network Proximity Estimation mechanism for implementing a control-plane policy, specifically the broker selection policy for a publish-subscribe system (\cref{fig:nw_proximity_pubsub_e2e_latency}). This control-plane policy selects the broker to host a given topic such that the end-to-end message delivery latency for all publisher-subscriber pairs is under the latency threshold for the topic, as shown in \cref{fig:nw_proximity_pubsub_e2e_latency}. As described in \cref{algo:pubsub_nw_prox_example}, it iterates over all the potential brokers and computes the worst-case communication latency if the topic is hosted on each of those brokers. The worst-case communication latency is the sum of the maximum publisher-broker network latency and the maximum broker-subscriber network latency. The sum of worst-case communication latency and processing latency on the broker gives the worst-case end-to-end latency for that topic. All the brokers that have the worst-case end-to-end latency under the topic-specific threshold are selected as candidates. The policy finally selects the candidate currently serving the lowest message rate among all candidates to ensure load balancing among brokers.

\begin{figure}
\centering
\includegraphics[width=0.6\linewidth]{figures/mechanisms/nw_proximity/pubsub_e2e_latency.pdf}
\label{fig:nw_proximity_pubsub_e2e_latency}
\caption{An instance of publish-subscribe service as an exemplary platform-service that requires information about network communication latency to compute the end-to-end latency experienced by the application. In this example, the end-to-end latency is the message delivery latency from any producer to all consumers for a given topic. The end-to-end latency is given by the sum of the maximum network latency from any producer to the broker, the processing latency on the broker, and the maximum network latency from the broker to any consumer.}
\end{figure}

\begin{algorithm}
\caption{Broker selection policy for topic $T$ with end-to-end latency threshold $L_{th}$}
\label{algo:pubsub_nw_prox_example}
\begin{algorithmic}
\Require topic $T$
\Require latency threshold $L_{th}$
\State $prod \gets \text{ set of producers for }T$
\State $cons \gets \text{ set of consumers for }T$
\State $candidates \gets \{\}$
\For{$\text{each broker } b$} 
    \State $nw\_lat \gets max_{p \in prod} \dfrac{1}{2} \cdot NetworkRTT \left( p, b \right) + max_{c \in cons} \dfrac{1}{2} \cdot NetworkRTT \left( c, b \right)$
    \State $e2e \gets nw\_lat + proc\_latency \left( b \right)$
    \If{$e2e \leq L_{th}$}
        \State $candidates \gets candidates \cup \{b\}$
    \EndIf
\EndFor
\Return broker in $candidates$ with lowest $msg\_rate$
\end{algorithmic}
\end{algorithm}

\section{End-to-End Monitoring Mechanism}
Situation-awareness applications require that the end-to-end latency and spatial affinity requirements are satisfied for the entire lifetime of the application, so that correct functionality can be guaranteed. However, given the constant mobility of end-clients, these requirements are likely to be violated repeatedly and frequently. For instance, a car moving away for the Edge Site currently serving the vehicle-local processing component would incur higher end-to-end processing latency. Similarly, a vehicle that moves out of the geographical area being served by the current region-level application component would receive fused sensor data updates that no longer pertain to its spatial context. Hence, the control-plane of the platform services are required to constantly monitor the running applications for such violations, perform root-cause analysis for determining the cause of the violation, and take appropriate reconfiguration action(s) to resolve the violation. Examples of such a reconfiguration would be the migration of the application component to an Edge Site that is closer (in terms of network proximity) to the end-client to ensure end-to-end latency satisfaction, or the re-mapping of the end-client to an application component that has the same spatial context as the end-client to satisfy the spatial affinity requirement.
\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{figures/mechanisms/monitoring/pipeline_latencies}
\caption{A breakdown of the end-to-end processing latency of an application pipeline into its constituent latencies.}
\label{fig:pipeline_latencies}
\end{figure}
\par Thorough monitoring of applications running atop platform services is a non-trivial task because there can be multiple sources of performance violation. For instance, as shown in \cref{fig:pipeline_latencies}, the observed end-to-end latency of the application instance is a sum of the queuing, execution and downstream communication latencies of each operator in the pipeline.  Thus, in order to detect a violation of the end-to-end latency requirement and identify the root-cause requires the monitoring of all the component latency metrics as independent timeseries and their aggregation and analysis. Each platform service has a notion of an \textit{application unit}, which represents an independent set of system entities that do not affect the performance of entities not within the given unit. An example of an application unit is a publish-subscribe topic, which consists of all the producers, consumers and broker hosting that topic. Furthermore, each platform service has its own set of metrics that need to be monitored for each application unit. These metrics then need to be aggregated and analyzed in a platform service-specific way as well to detect violations. The monitoring mechanism needs to scale with an increasing number of application instances being monitored and impose low overhead on the applications and infrastructure.

\subsubsection{Shortcomings of previous work}
\par Application monitoring has been an active area of work in the cloud computing space with a number of research projects and commercial offerings available. In the context of cloud computing, applications typically don't possess end-to-end latency requirements and the goal of monitoring is to ensure that the tail latency of a given service (that could consist of multiple instances) doesn't increase significantly. Platforms such as Prometheus \cite{prometheus} and Monasca \cite{monasca} don't support the aggregation of multiple metrics to study the behavior of end-to-end latency. Furthermore, their architecture is a fully centralized one, resulting in the use of network bandwidth to send the monitoring data to the cloud.
\par Monitoring systems designed for edge computing environments also suffer from limitations, wherein they aggregate measured metrics over geographical regions instead of aggregating multiple metrics pertaining to the same application instance \cite{fmone, gonccalves2021dynamic}. Such systems are not capable of detecting violations of end-to-end performance constraints. Previous contributions to building systems services such as publish-subscribe systems \cite{emma} and distributed application runtimes \cite{foglets} consist of monitoring subsystems that only monitor the the network connectivity between a client and the edge site that it is connected directly to. Since these solutions don't consider end-to-end latency as the primary metric, they would result in triggering more reconfigurations to keep clients connected to the closest edge site than needed.
\begin{figure}
\centering
\includegraphics[width=0.75\linewidth]{figures/mechanisms/monitoring/migrations_count.png}
\caption{Ratio of the number of migrations triggered by Greedy monitoring scheme versus those triggered by an optimal policy. The end-to-end latency threshold is varied along the x-axis.}
\label{fig:migration_count}
\end{figure}
\par In the following we show an evaluation of a greedy monitoring approach that aims to minimize the last-mile network latency between the client and the edge-site it directly connects to. The metric of interest is the number of migrations triggered by such a policy compared to an optimal policy that monitors the end-to-end latency and only triggers a migration when the observed end-to-end latency violates the application's threshold. We simulate the mobility of 200 independent clients in the city of Shanghai using the Random Waypoint mobility model and track the number of migrations triggered by both the greedy and optimal policies. In \cref{fig:migration_count}, we present the ratio of the migrations triggered by the greedy policy and that by the optimal policy with increasing end-to-end latency bound. We see that as the end-to-end latency bound increases, the relative number of migrations triggered by greedy policy becomes much higher. This is because, although the greedy policy triggers the same number of migrations as it is independent of the end-to-end latency constraint, the number of migrations triggered by the optimal policy decreases with an increase in the threshold. This necessitates that any useful and efficient monitoring system should consider multiple metrics together that pertain to different components of the end-to-end latency. Violation detection needs to happen for each application-level unit, or application instance, such as a publish-subscribe topic, or one instance of an application pipeline (as shown in \cref{fig:pipeline_latencies}). Hence, metrics that belong to the same application unit should be aggregated together for violation detection and other control-plane policies.

\subsection{Interface provided to control-plane}
The following abstractions are provided to the control-plane of a platform service when using the distributed end-to-end monitoring mechanism.
\begin{itemize}
\item \textbf{RegisterAppUnit.} Registration of an Application Unit when it is created. For instance, when a new topic is created in a publish-subscribe system, the topic should be registered as a new Application Unit. The Monitoring mechanism uses the identifier of the application unit to identify the metrics that pertain to it, and is thus able to perform end-to-end aggregation on them.
\item \textbf{RegisterMetric.} System components can register custom metrics and publish measurements to them. The metrics are annotated with the application unit that they correspond do, as well as custom platform-specific tags. The following snippet shows an example, which measures the network latency of an instance of the pipeline stage shown in \cref{fig:app_pipeline} to its downstream component. In the given platform service, the application unit that a metric belongs to is the downstream-most application component, or the root component. In this example, the application unit is the application component instance $L_0$.
\begin{minted}{yaml}
-   entity_id : "L10"
    entity_type: "APP"
    app_unit: "L0"
    metric: "net_latency"
\end{minted}
Each metric stream is represented an ordered sequence of measurements along with their timestamps as shown in \cref{eq:metric_stream}.
\begin{equation}
M = \left[ \cdots , \left( t, v \right) , \cdots \right] \text{ where } 0 < t < \infty
\label{eq:metric_stream}
\end{equation}

\item \textbf{AlignMetrics.} The control-plane policy can time-align the measurements for a specific metric into well-defined buckets. This interface is necessary as a preprocessing step before multiple metrics can be aggregated together. Since different metrics are likely to be recorded at different instants in time, aligning them in time allows the values from different metrics to be processed together, such that two values from different metrics that were collected at roughly the same time can be processed together. \cref{fig:time_alignment} illustrates the time alignment of a metric for a fixed size time bucket, and all measurements within a given bucket are aggregated by taking an average over them. The platform service is supposed to provide the bucket size $B$ and the aggregation function $func$ for aggregating a metric $M$.
\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/mechanisms/monitoring/time_alignment}
\caption{Illustration of time alignment of metrics. The individual measurements within each time bucket are aggregated using average function to generate the measurement for that bucket. The time alignment function takes as input a metric stream and time bucket size and returns another stream.}
\label{fig:time_alignment}
\end{figure}
\begin{multline}
ALIGN^{func}_{B} \left( M \right) = \left[ \cdots , \left( t, v \right) , \cdots \right] \text{ where } t = n\cdot B \text{ and } 0 < n < \infty \\ \text{ and } v = func \left( \{ v' : \left( t', v'\right) \in M \text{ and } B\cdot \left( n-1\right) < t' \leq n \cdot B \} \right)
\label{eq:metric_stream}
\end{multline}

%\item Platform-specific definition of the system entity processing all the metrics of a given application unit for performing aggregation and violation detection. For instance, in the case of a publish-subscribe system, all the metrics pertaining to a specific topic can be processed at the broker serving that topic.
\item \textbf{ProcessMetrics. }This interface allows platform-services to specify logic for metrics aggregation and violation detection policies. The logic should be able to query metrics using the aforementioned tags, generate new aggregated metrics, and call an endpoint in the platform service's control-plane to notify it of a violation and provide information about the root cause. The policy should also be able to access certain configurations of the platform service, such as the mapping of child application components to parent application component for the application orchestrator, or the mapping of topics to brokers in the case of publish-subscribe system. Information about such configurations is useful for knowing which metrics need to be aggregated together.
\item \textbf{QueryMetrics. }The logic provided to the end-to-end monitoring mechanism via the ProcessMetrics interface should be able to query metrics using their labels.
\end{itemize}

\subsection{Demonstration of using end-to-end monitoring for control-plane policy of a platform service}
In this section, we show how the end-to-end monitoring mechanism will be used for implementing the control-plane policy for detecting violation of end-to-end processing latency for a geo-distributed application orchestrator system. For this example, we would restrict the discussion to clients and application instances for a single application unit, but the approach generalizes to multiple units.
\begin{enumerate}
\item The library of the platform service running alongside each client and backend application instance generates two metrics - namely the processing latency and network latency to the immediately downstream application component instance (called \textit{parent}). For application component $e$ denote them as $proc_e$ and $net_e$.\\
\begin{minipage}{0.45\textwidth}
\begin{minted}{yaml}
-   entity_id : "L20"
    entity_type: "CLIENT"
    app_unit: "L0"
    metric: "proc_latency"
\end{minted}
\end{minipage}%
\hfill
\begin{minipage}{0.45\textwidth}
\begin{tabular}{p{\textwidth}}
\begin{minted}{yaml}
-   entity_id : "L10"
    entity_type: "APP"
    app_unit: "L0"
    metric: "net_latency"
\end{minted}
\end{tabular}
\end{minipage}

The above listings show definitions of metrics corresponding to the processing and network latency of the application component instance $L_{20}$ and $L_{10}$ respectively in the application instance shown in \cref{fig:app_pipeline}. Both these component instances pertain to the application unit which is associated with the "root" application component instance $L_0$. 
\item The platform service requests for time-alignment of all the metric streams using the function $ALIGN^{AVG}_{5secs}$. We denote the aligned version of metric stream $M$ as $M*$.
\item The control-plane policy for detecting violations of end-to-end processing latency performs calculations for each of the root application instances, which we denote as $L_0$. It selects the latency metrics corresponding to the clients that are connected to $L_0$ as shown in \cref{fig:app_pipeline}. It uses a query like the following to select the latency metrics for all the clients that are connected to the root application component $L_0$.
\begin{minted}{sql}
SELECT METRICS WITH entity_type="CLIENT" AND app_unit="L0"
\end{minted}
\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{figures/mechanisms/monitoring/app_pipeline.png}
\caption{Schematic of a typical situation-awareness application. The application model resembles a tree, with the leaf vertices corresponding to clients. Each vertex has a parent vertex except the root vertex.}
\label{fig:app_pipeline}
\end{figure}
\item The above set of metrics are then grouped by the field $entity\_ID$ because they contain both processing latency and network latency metrics for each client. Grouping them by $entity\_ID$ allows the policy to separate the metrics of a specific client from other clients.
\item For each client component $c$, the policy computes the set of application components $S_c$ that process data generated by $c$. The following pseudocode illustrates this computation.
\begin{algorithmic}
\State $n \gets c$
\State $S_c \gets \{\}$
\While{$n \neq \phi$} 
    \State $S_c \gets S_c \cup \{ n \}$
    \State $n \gets M \left[ n \right]$
\EndWhile
\end{algorithmic}
We use the notation $M \left[ n \right]$ to denote the downstream application component reading the output of $n$. The downstream application component for the root component $L_0$ is designated to be null ($\phi$). This information is obtained from the application orchestrator platform service's control-plane metadata about application mapping.

\item For each client $c$, now it is possible to compute a time-aligned metric stream which records the end-to-end processing latency for $c$.
\begin{equation}
E2E_c^* = \sum_{e \in S_c} proc_e^* + net_e^*
\end{equation}
For those clients whose end-to-end latency estimation exceeds the application's threshold, a reconfiguration action is triggered. The objective of the reconfiguration action is determined by root-cause analysis, wherein the relative contribution of each of the component latencies toward the violation end-to-end latency violation is analyzed. In case the violation is most significantly caused by an increase in communication latency between an upstream-downstream application component pair $\left( l1, l2 \right)$ then $l1$ is re-mapped to another downstream component instance that has a better network latency. Similarly, if the root-cause is an increase in processing latency at a certain component instance $l$, then it is either allocated more resources to bring down the processing latency. If resource allocation cannot be increased due to capacity constraints, a fraction of its immediately upstream component instances are mapped to another instance so as to reduce the workload on $l$.
\end{enumerate}

