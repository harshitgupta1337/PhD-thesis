\chapter{Conclusion and Future Directions}
\label{sec:conclusion}
\section{Conclusion}
\section{Future Directions}
\subsection{Federation of Control Plane}
This dissertation highlights the efficacy of the proposed mechanisms in the context of platform services that have a centralized control plane architecture, wherein policy decisions are made by a centralized entity. Note that by a centralized entity, we do not refer to a monolith. The centralized control plane could consist of multiple components, with replication to ensure fault tolerance. In this context, a centralized architecture refers to the fact that the control plane components are located in a single facility which is separated from all the data plane components by the wide area network. Such a design makes the coordination between control and data plane inefficient due to repeated WAN traversals to accomplish tasks such as application instance deployment, data migration, etc.
\par In a federated control plane architecture, there would be multiple controllers for a given platform service. Each controller would manage a subset of Edge sites, which could overlap with the set of sites managed by another controller. This overlap of sites is essential to ensure that in the event of the failure of a controller, the sites served by the failing controller can be managed by another one. When reserving Edge resource capacity for data or compute placement, the controller making this decision would need to synchronize with the other controllers managing the affected Edge sites to ensure that their infrastructure state is consistently updated. A distributed synchronization protocol such as two-phase commit can be used for this purpose.
\par A federated architecture allows the control plane to be located close to the Edge sites that host the data plane components, while still allowing the control policy decisions to be made against global infrastructure state. In our previous work \cite{oneedge}, we have shown how introducing decentralization in the control plane results in better response times. This federation, however, comes at a cost in terms of how and when to handover monitoring data from one controller instance to another. This challenges can be addressed by once again leveraging the fact that the goal of a federated design is to have the multiple controller instances close to the Edge site they are managing. Hence, a shared data store for monitoring data could be utilized in which case explicit migration of data between controller instances can be avoided.

\subsection{Extending to Other Platform Services}
A widely used platform service in the Edge computing space is Functions as a Service (FaaS). FaaS is a useful paradigm for application development at the Edge because it quickly scales in and out according to ingress workload, and hence suits the scarce nature of Edge resources. Contemporary FaaS platforms such as Knative \cite{knative}, Apache OpenFaaS \cite{openfaas} and OpenWhisk \cite{openwhisk} consist of a centralized gateway, which receives ingress requests and distributes it to one or more workers of the requested function. Since they have been designed for datacenters, the communication latency between clients and the ingress gateway, and that between the gateway and worker nodes is predictably low. This assumption does not hold true in an Edge computing setting.
\par Therefore, to operate a geo-distributed FaaS platform, three main changes to the typical architecture are needed. (1) Firstly, instead of a single ingress gateway, there need to be multiple geo-distributed gateways, with at least one gateway per Edge site hosting workers. This is to ensure that function invocation requests do not suffer from unnecessary network traversal overhead. (3) Secondly, the dispatch of requests from the ingress gateway to workers should be done to ensure that the sum of communication and execution latencies does not exceed the application's end-to-end processing latency constraint. The estimation of communication latency between gateway and worker node can use the network proximity estimation mechanism proposed in this dissertation, while the current execution latency profile can be inferred using the end-to-end monitoring mechanism. (3) Finally, worker nodes on Edge sites can get overloaded due to unpredictable workload surges. In such a scenario, the overloaded worker node would offload ingress request to other worker nodes which might have spare processing capacity. The choice of offloading target is again dependent on the communication latency between the offloader and offloadee, such that the end-to-end latency constraint is not violated. For the estimation of communication latency, the network  proximity estimation mechanism can be used.

\subsection{Incorporating Radio Network Information into Control Plane Policy Decisions}
This dissertation assumes that the access network link via a cellular tower's radio interface is reliable and offers stable network latency and bandwidth. However that is a simplifying assumption that does not hold in real-world scenarios. The performance of the access link varies due to interference with other user devices, user mobility or due to effects from the weather. Drops in channel quality causes packet loss, which affects both latency and throughput. The Mobile Edge Computing (MEC) standard by the European Telecommunications Standards Institute (ETSI) includes Radio Network Information Service. The goal of the service is to allow authorized MEC application instances to consume RAN level information, such as UE channel quality indications and location updates, which they can utilize to offer enhanced services and optimize performance. Earlier this information was only available to the telecommunication network operator, who used it for allocating wireless spectrum resources to users. However, with the advent of MEC and the concomitant co-location of the control planes of the network and application stack, this information can also be shared with the application control plane stack.
\par The information provided by the RNIS can be used in a number of ways to offer reliable and stable performance to applications. For instance, Tan et al. \cite{tan2018radio} utilize the network throughput estimates from the RNIS to inform the cache update strategy for a Video-on-Delivery caching service running on the Edge. The key idea is to cache video segments of particular bitrates only, i.e., those bitrates that cane be delivered to the user with good performance given the current network conditions. In a similar vein, Li et al. \cite{li2017mobile} propose a video delivery scheme for user devices that are connected to multiple access networks. The control plane of the proposed system periodically analyzes the status of the multiple access networks and decides which one to use for the delivery of a certain video segment so as to ensure optimal client experience. The control plane obtains network status using RNIS. 

\subsection{Monitoring Bandwidth as a Metric of Interest}
This dissertation uses latency as the foremost performance metric for situation-awareness applications. However, for typical Edge computing applications, network bandwidth is also an important performance metric that should be taken into account when making control policy decisions. However, bandwidth monitoring is complicated as it involves sending packets through potentially already congested network links, which can severely affect application performance. \todo{Expand this section}.

\subsection{Proactive Violation Detection Policies}