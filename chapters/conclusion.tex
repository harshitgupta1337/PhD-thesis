\chapter{Conclusion and Future Directions}
\label{sec:conclusion}
This dissertation proposed three mechanisms that aid the control plane policies of edge-centric platform services to satisfy the requirements of situation-awareness applications. This chapter first presents a summary of the main contributions of this dissertation and then presents future directions for research in this domain.

\section{Conclusion}
Situation-awareness applications consist of a sense-process-actuate control loop, which sensed data from the environment, processes it and performs actions based on the extracted insights. These applications require low response time from the backend to ensure that the performed actions are in real-time with respect to changes in the surrounding environment. Furthermore, they also exhibit spatial affinity, wherein multiple nearby clients need to be served by the same application instance. These requirements are not fulfilled by the contemporary offerings of cloud-based platform services. Hence, new mechanism are needed to facilitate edge-centric control policy decisions in these platform services. 
\par This dissertation introduces the proposed mechanisms in \cref{sec:mechanisms}, wherein the necessity of each proposed mechanism are highlighted by quantitatively analyzing the performance of the state of the art control policies. It then formalizes the interface provided by the mechanism to the control policies of platform services, and demonstrates the utility of each mechanism in the context of a platform service's control policy. \cref{sec:design_space_exploration} then performs a design-space exploration of each mechanism. It presents candidate design choices, explains the metrics of interest and compares the candidate design choices against each other. 
\par The dissertation then goes on to demonstrate the utility of the proposed mechanism in the context of three platform services in \cref{sec:epulsar}, \cref{sec:oneedge}, \cref{sec:epulsar} and \cref{sec:fogstore}. \cref{sec:epulsar} presents \epulsar{}, which is an edge-centric publish-subscribe system that offers end-to-end message delivery latency guarantees to applications. It utilizes the network proximity estimation mechanism for selecting the right broker for hosting a pub-sub topic. The mechanism is used for estimating the message delivery latency for a topic if a particular candidate broker is chosen for hosting the topic. It also utilizes the distributed end-to-end monitoring mechanism to monitor the observed latency by clients and trigger a topic migration to another broker if the latency requirement is violated.
\par \cref{sec:oneedge} presents an application orchestrator system for situation-awareness applications. It supports applications that have response time requirements as well as spatial affinity requirements. It utilizes the network proximity estimation mechanism to perform latency-sensitive placement of application components on the infrastructure and the dynamic spatial context management mechanism for mapping clients to application instances while satisfying spatial affinity requirements. It leverages the end-to-end monitoring mechanism for detecting violations of response-time requirement, identifying the root-cause of the violation and triggering the right reconfiguration action. Through end-to-end evaluations of realistic application workload on a realistic infrastructure topology, we have shown that \oneedge{} is able to meet the requirements of situation-awareness applications.
\par \cref{sec:fogstore} presents FogStore, which is a key-value store built for highly geo-distributed infrastructures with edge-centric context-aware replica placement and quorum selection policies -- for both of which FogStore leverages the Dynamic Spatial Context Management mechanism. FogStore's replica placement is done taking the low-latency requirement of data-access into account, while also making the placement tolerant to geographically correlated failures. It, creates two types of replicas, ones which are located in proximity to the clients for low latency and ones in remote location for fault-tolerance. The quorum selection policy leverages the fact that the context of clients determine the degree of consistency expected when querying the state of a situation-awareness application. Hence queries from clients in the vicinity of the queried item, which require consistent data access, have a majority of the proximal replicas in quorum, while those from remote clients are offered eventual consistency. FogStore uses the Dynamic Spatial Context Management mechanism to perform both replica placement and quorum selection. We show the performance of the proposed policies by implementing them on Apache Cassandra and using YCSB to stress-test the system. Evaluations show that the proposed policies are able to achieve a throughput and latency comparable to eventually consistent systems, while still guaranteeing serializability guarantees on relevant data-items to clients.

\section{Future Directions}
We now discuss ideas for future work that follow from the contributions of this dissertation.
\subsection{Federation of Control Plane}
This dissertation highlights the efficacy of the proposed mechanisms in the context of platform services that have a centralized control plane architecture, wherein policy decisions are made by a centralized entity. Note that by a centralized entity, we do not refer to a monolith. The centralized control plane could consist of multiple components, with replication to ensure fault tolerance. In this context, a centralized architecture refers to the fact that the control plane components are located in a single facility which is separated from all the data plane components by the wide area network. Such a design makes the coordination between control and data plane inefficient due to repeated WAN traversals to accomplish tasks such as application instance deployment, data migration, etc.
\par In a federated control plane architecture, there would be multiple controllers for a given platform service. Each controller would manage a subset of Edge sites, which could overlap with the set of sites managed by another controller. This overlap of sites is essential to ensure that in the event of the failure of a controller, the sites served by the failing controller can be managed by another one. When reserving Edge resource capacity for data or compute placement, the controller making this decision would need to synchronize with the other controllers managing the affected Edge sites to ensure that their infrastructure state is consistently updated. A distributed synchronization protocol such as two-phase commit can be used for this purpose.
\par A federated architecture allows the control plane to be located close to the Edge sites that host the data plane components, while still allowing the control policy decisions to be made against global infrastructure state. In our previous work \cite{oneedge}, we have shown how introducing decentralization in the control plane results in better response times. This federation, however, comes at a cost in terms of how and when to handover monitoring data from one controller instance to another. These challenges can be addressed by once again leveraging the fact that the goal of a federated design is to have the multiple controller instances close to the Edge site they are managing. Hence, a shared data store for monitoring data could be utilized in which case explicit migration of data between controller instances can be avoided.

\subsection{Extending to Other Platform Services}
A widely used platform service in the Edge computing space is Functions as a Service (FaaS). FaaS is a useful paradigm for application development at the Edge because it quickly scales in and out according to ingress workload, and hence suits the scarce nature of Edge resources. Contemporary FaaS platforms such as Knative \cite{knative}, Apache OpenFaaS \cite{openfaas} and OpenWhisk \cite{openwhisk} consist of a centralized gateway, which receives ingress requests and distributes it to one or more workers of the requested function. Since they have been designed for datacenters, the communication latency between clients and the ingress gateway, and that between the gateway and worker nodes is predictably low. This assumption does not hold true in an Edge computing setting.
\par Therefore, to operate a geo-distributed FaaS platform, three main changes to the typical architecture are needed. (1) Firstly, instead of a single ingress gateway, there need to be multiple geo-distributed gateways, with at least one gateway per Edge site hosting workers. This is to ensure that function invocation requests do not suffer from unnecessary network traversal overhead. Secondly, the dispatch of requests from the ingress gateway to workers should be done to ensure that the sum of communication and execution latencies does not exceed the application's end-to-end processing latency constraint. The estimation of communication latency between gateway and worker node can use the network proximity estimation mechanism proposed in this dissertation, while the current execution latency profile can be inferred using the end-to-end monitoring mechanism. Finally, worker nodes on Edge sites can get overloaded due to unpredictable workload surges. In such a scenario, the overloaded worker node would offload ingress request to other worker nodes which might have spare processing capacity. The choice of offloading target is again dependent on the communication latency between the offloader and offloadee, such that the end-to-end latency constraint is not violated. For the estimation of communication latency, the network  proximity estimation mechanism can be used.

\subsection{Incorporating Radio Network Information into Control Plane Policy Decisions}
This dissertation assumes that the access network link via a cellular tower's radio interface is reliable and offers stable network latency and bandwidth. However that is a simplifying assumption that does not hold in real-world scenarios. The performance of the access link varies due to interference with other user devices, user mobility or due to effects from the weather. Drops in channel quality causes packet loss, which affects both latency and throughput. The Mobile Edge Computing (MEC) standard by the European Telecommunications Standards Institute (ETSI) includes Radio Network Information Service. The goal of the service is to allow authorized MEC application instances to consume RAN level information, such as UE channel quality indications and location updates, which they can utilize to offer enhanced services and optimize performance. Earlier this information was only available to the telecommunication network operator, who used it for allocating wireless spectrum resources to users. However, with the advent of MEC and the concomitant co-location of the control planes of the network and application stack, this information can also be shared with the application control plane stack.
\par The information provided by the RNIS can be used in a number of ways to offer reliable and stable performance to applications. For instance, Tan et al. \cite{tan2018radio} utilize the network throughput estimates from the RNIS to inform the cache update strategy for a Video-on-Delivery caching service running on the Edge. The key idea is to cache video segments of particular bitrates only, i.e., those bitrates that cane be delivered to the user with good performance given the current network conditions. In a similar vein, Li et al. \cite{li2017mobile} propose a video delivery scheme for user devices that are connected to multiple access networks. The control plane of the proposed system periodically analyzes the status of the multiple access networks and decides which one to use for the delivery of a certain video segment so as to ensure optimal client experience. The control plane obtains network status using RNIS. 

\subsection{Monitoring Bandwidth as a Metric of Interest}
This dissertation uses latency as the foremost performance metric for situation-awareness applications. However, for typical Edge computing applications, network bandwidth is also an important performance metric that should be taken into account when making control policy decisions. Several previous work propose bandwidth-aware control plane policies for managing applications that require high network bandwidth. Dedas \cite{dedas} is an online deadline-aware task dispatching and scheduling algorithm that takes into account the network bandwidth and propagation latency between Edge sites. The objective of Dedas is to control the latency of task execution such that the deadline of the task can be met. It selects Edge sites for dispatching tasks such that both the transmission latency and propagation latency are low. Lavea \cite{lavea} is a latency-aware video analytics platform which is designed for operation on Edge infrastructure. Lavea offloads video analytics tasks between clients and Edge nodes and also facilitates coordination between multiple Edge nodes. Based on the chosen offloading strategy, it allocates bandwidth among clients to ensure that the deadlines of tasks are met. The system consists of a monitoring service that periodically measures the latency and available bandwidth of the wireless link (4G LTE/5G/WiFi) which is used to make task offloading decisions. However, bandwidth monitoring is complicated as it involves sending packets through potentially already congested network links, which can severely affect application performance. The spatial and temporal variations in bandwidth can also be predicted in advance by using techniques such as Foresight \cite{foresight}.

\subsection{Proactive Violation Detection Policies}
The control policies discussed in this dissertation for detecting violation of application requirements and root-cause analysis act in reaction to a violation. Therefore, clients will always go through a transient phase of requirement violation before the platform service can detect and alleviate it. However, proactive policies can be integrated into the control plane to proactively identify situations in which a violation is expected to occur in the near-future and trigger a re-configuration action. Such proactivity in the control plane would significantly minimize or even eliminate violations. The processing latency on application components can be analyzed to identify short-term trends which can help trigger a partitioning of client workload in advance. Systems like Foresight \cite{foresight} can be used to predict when the network quality is going to deteriorate in the future and trigger a migration.