\begin{summary}

Situation-awareness applications require low-latency response and high network bandwidth, hence benefiting from geo-distributed Edge infrastructures. \soutnew{These applications rely on several platform services}\new{The developers of these applications typically rely on several platform services}, such as Kubernetes, Apache Cassandra and Pulsar, for managing their compute and data components \new{across the geo-distributed Edge infrastructure}. \soutnew{These platform services are responsible for scheduling compute components and placing data across the geo-distributed Edge infrastructure, that has strong implications on the applicationsâ€™ response time.} \new{Situation-awareness applications impose peculiar requirements on the compute and data placement policies of the platform services.} \soutnew{Hosting situation-awareness applications on Edge infrastructure poses novel requirements on the platform services.} Firstly, the processing logic of these applications is closely tied to the physical environment that it is interacting with. Hence, the access pattern to compute and data exhibits strong spatial affinity. Secondly, the network topology of Edge infrastructure is heterogeneous, wherein communication latency forms a significant portion of the end-to-end compute and data access latency. \new{Therefore, the placement of compute and data components has to be cognizant of the spatial affinity and latency requirements of the applications. However, clients of situation-awareness applications, such as vehicles and drones, are typically mobile -- making the compute and data access pattern dynamic and complicating the management of data and compute components. Constant changes in the network connectivity and spatial locality of clients due to client mobility results in making the current placement of compute and data components unsuitable for meeting the latency and spatial affinity requirements of the application. Constant client mobility necessitates that client location and latency offered by the platform services be continuously monitored to detect when application requirements are violated and to adapt the compute and data placement.}\soutnew{Finally, clients of situation-awareness applications are inherently mobile, necessitating continuous adaptation of compute and data placement decisions to ensure latency requirements are satisfied while adhering to spatial affinity requirements.}
\new{The control and monitoring modules of off-the-shelf platform services do not have the necessary primitives to incorporate spatial affinity and network topology awareness into their compute and data placement policies. The spatial location of clients is not considered as an input for decision-making in their control modules. Furthermore, they do not perform fine-grained end-to-end monitoring of observed latency to detect and adapt to performance degradations due to client mobility.}
\soutnew{The control planes of off-the-shelf platform services do not have the necessary primitives to incorporate spatial affinity and network topology awareness into the compute and data orchestration policies. They also do not perform fine-grained end-to-end monitoring of application response times to detect and adapt to performance degradations due to client mobility.} \soutnew{This dissertation presents three mechanisms that inform the compute and data placement policies for platform services, so that application performance requirements can be met.}\new{This dissertation presents three mechanisms that inform the compute and data placement policies of platform services, so that application  requirements can be met.}
\begin{itemize}
\item \new{M1: Dynamic Spatial Context Management for system entities -- clients and data and compute components -- to ensure spatial affinity requirements are satisfied.}\soutnew{Distributed spatial context management for system entities - including clients and data/compute components to ensure spatial affinity requirements are satisfied.}
\item \new{M2: Network Proximity Estimation to provide topology-awareness to the data and compute placement policies of platform services.}\soutnew{Topology awareness through scalable network proximity estimation among clients and Edge sites.}
\item \new{M3: End-to-End Latency Monitoring to enable collection, aggregation and analysis of per-application metrics in a geo-distributed manner to provide end-to-end insights into application performance.} \soutnew{Fine-grained monitoring and aggregation of per-application metrics in a geo-distributed manner to provide end-to-end insights into application performance.}
\end{itemize}
\new{The thesis of our work is that the aforementioned mechanisms are fundamental building blocks for the compute and data management policies of platform services, and that by incorporating them, platform services can meet application requirements at the Edge. Furthermore, the proposed mechanisms can be implemented in a way that offers high scalability to handle high levels of client activity.}
\soutnew{The thesis of our work is that the aforementioned mechanisms are essential for meeting the quality of service guarantees for situation-awareness applications on geo-distributed infrastructures.} We demonstrate by construction the efficacy and scalability of the proposed mechanisms for building dynamic compute and data orchestration policies by incorporating them in the control and monitoring modules of three different platform services. Specifically, we incorporate these mechanisms into a topic-based publish-subscribe system (ePulsar), an application orchestration platform (OneEdge), and a key-value store (FogStore). We conduct extensive performance evaluation of these enhanced platform services to showcase how the new mechanisms aid in dynamically adapting the compute/data orchestration decisions to satisfy performance requirements of applications.
\end{summary}